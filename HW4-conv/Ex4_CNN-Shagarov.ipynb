{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6dWwjPmrePT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load the dataset\n",
    "Don't change this code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Seed everything\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_random_seed(323212)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data. Don't change this code\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "\n",
    "# CIFAR10 z-normalization const https://github.com/facebookarchive/fb.resnet.torch/issues/180\n",
    "cifar10_mean = (0.491, 0.482, 0.447)\n",
    "cifar10_std = (0.247, 0.244, 0.262)\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # PIL Image to Pytorch tensor\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "    # https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20normalize#torchvision.transforms.Normalize\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(\"/datasets\", train=True, transform=transform, download=True)\n",
    "\n",
    "# Load class names\n",
    "with open(\"/datasets/cifar-10-batches-py/batches.meta\", 'rb') as infile:\n",
    "    cifar_meta = pickle.load(infile)\n",
    "labels = cifar_meta['label_names']\n",
    "\n",
    "# Split dataset into train and val\n",
    "train_ds, val_ds, _ = random_split(dataset, [10000, 2000, 38000])\n",
    "batch_size = 256\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5zVN1kHd43W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Function for accuracy checking\n",
    "\n",
    "Don't change this code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OrxQRvfbahxH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def validate(model, testloader, device=\"cpu\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Добавим подсчет лосса\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader):\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images.to(device))\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "    return correct / total, loss / len(testloader)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX-f8_6HrngE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implement CNN class for CIFAR10\n",
    "\n",
    "**In constructor**\n",
    "\n",
    "Define 2 - 3 convolutional layers \n",
    "\n",
    " https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "with corresponding in/out dimensions W_out = 1 + ((W_in - F + 2*P) / S)\n",
    "\n",
    "\n",
    "Also define max pooling : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "and fully connected layers: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "\n",
    "\n",
    "**In forward**\n",
    "\n",
    "Write code for forward pass.\n",
    "Remember that first dimension is the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_zLprG7kk-Q9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, class_nums=10, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        # Что-то вроде vgg блока\n",
    "        self.vgg1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            act,\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            act,\n",
    "            # nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "            act,\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Линейный классификатор\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 64, 1024),\n",
    "            act,\n",
    "            nn.Linear(1024, class_nums)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "TwoLayerCNN(\n  (vgg1): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=1024, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=1024, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwoLayerCNN()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3DAyLmoryLp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl3wwvZXsgfq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I will use wandb instead of TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vIhmoh_Fsfsa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"cv-convs\", entity=\"dmitysh\")"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdmitysh\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\dm1tr\\Desktop\\ml-cv-seminars\\HW4-conv\\wandb\\run-20221106_210635-2gtwka0o</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/dmitysh/cv-convs/runs/2gtwka0o\" target=\"_blank\">snowy-deluge-86</a></strong> to <a href=\"https://wandb.ai/dmitysh/cv-convs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dmitysh/cv-convs/runs/2gtwka0o?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x1f0550ce380>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9HPW9T2tSRp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement training loop\n",
    "\n",
    "- Create optimizer,\n",
    "- Save loss and accuracy values into tensorboard log\n",
    "- Use GPU to speedup training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qcJFI3hTOJlD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "\n",
    "set_random_seed(9696)\n",
    "\n",
    "\"\"\"\n",
    "  Send model to GPU\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoLayerCNN(10)\n",
    "model = model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "\"\"\"\n",
    "  Adding criterion and setting train mode on\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\"\n",
    "  Setup optimizer for your model\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=7e-4)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img_batch, labels_batch in tqdm(train_loader):\n",
    "        img_batch = img_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        output = model(img_batch)\n",
    "        loss = criterion(output, labels_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    accuracy, mean_val_loss = validate(model, val_loader, device)\n",
    "    mean_train_loss = train_loss / len(train_loader)\n",
    "    # scheduler.step(mean_val_loss)\n",
    "\n",
    "    \"\"\"\n",
    "      Write data to wandb\n",
    "    \"\"\"\n",
    "    wandb.log({\"epoch\": epoch,\n",
    "               \"train loss\": mean_train_loss,\n",
    "               \"val loss\": mean_val_loss,\n",
    "               \"val accuracy\": accuracy})\n",
    "\n",
    "    print(\"Epoch {} Loss {:.2f} Accuracy {:.2f}\".format(epoch, mean_train_loss, accuracy))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:13<00:00,  2.87it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1.99 Accuracy 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.32it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.64 Accuracy 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.33it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 1.48 Accuracy 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.08it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 1.38 Accuracy 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.35it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 1.31 Accuracy 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.54it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 1.25 Accuracy 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.35it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 1.19 Accuracy 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.09it/s]\n",
      "100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 1.14 Accuracy 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.45it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 1.09 Accuracy 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 1.05 Accuracy 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.42it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 1.00 Accuracy 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.36it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss 0.95 Accuracy 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.45it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 0.90 Accuracy 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.48it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss 0.86 Accuracy 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.41it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 0.82 Accuracy 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.36it/s]\n",
      "100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss 0.81 Accuracy 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.35it/s]\n",
      "100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 0.76 Accuracy 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.23it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss 0.71 Accuracy 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.68 Accuracy 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.34it/s]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss 0.64 Accuracy 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4tIFR5bwZFi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validate results on test dataset\n",
    "\n",
    "You must get accuracy above 0.65"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MM0pWYJlwibm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "test_dataset = datasets.CIFAR10(\"/datasets\",\n",
    "                                train=False,\n",
    "                                transform=dataset.transform,  # Transforms stay the same\n",
    "                                download=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "model.eval()\n",
    "\n",
    "accuracy = validate(model, test_loader, device)\n",
    "print(f\"Accuracy on test:{accuracy}\")\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test:(0.6093, 1.182271096110344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIvyaeSVsIl0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Place for brief conclusion:\n",
    "\n",
    "Получить аккураси >0.65 используя 2-3 сверточных слоя без тотального переобучения кажется непостижимой задачей.\n",
    "Я пробовал более 70 запусков (посмотреть можно тут : https://wandb.ai/dmitysh/cv-convs?workspace=user-dmitysh)\n",
    "Начал с двух сверточных слоев, потом добавил третий, что чуть улучшило качество.\n",
    "Менял число эпох.\n",
    "Пробовал различный learning rate, в какой-то момент прикрутил шедулер, но потом убрал, так как эффект был не супер сильный.\n",
    "Менял линейные слои в classifier, добавлял батч нормы и дропауты, причем как в линейные слои, так и после сверток.\n",
    "Лучшее, что у меня получилось было в районе 0.63.\n",
    "\n",
    "Кажется, что модели просто не хватает данных для обучения (всего 10к).\n",
    "И качество сильно меняется от сида к сиду при разбиении.\n",
    "В какой-то момент обучил простейшую модель на 25к вместо 10к изображений и с легкостью получил почти 0.7 accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYMD7UT5BlAS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ideas for extra work\n",
    "\n",
    "---\n",
    "1. Evaluate the impact of the number and size of filters in convolutional layers on the accuracy. Кажется для такой маленькой картинки брать больше 3 на 3 точно не стоит. А вот число фильтров качество улучшало, пока не доходило до переобучения.\n",
    "\n",
    "2. Evaluate the impact of the convolutional layers count on the accuracy. Попробовал, третий слой улучшил качество по сравнению с 2-мя при остальном неизменном.\n",
    "\n",
    "\n",
    "3. Visualization something ... ссылка на запуски снизу\n",
    "# https://wandb.ai/dmitysh/cv-convs?workspace=user-dmitysh\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}